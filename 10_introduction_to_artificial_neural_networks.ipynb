{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To support both python 2 and python 3\n",
    "from __future__ import division, print_function, unicode_literals\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "\n",
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "CHAPTER_ID = \"ann\"\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True):\n",
    "    path = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID, fig_id + \".png\")\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format='png', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Artificial Neural Networks\n",
    "1. **인공신경망(ANN)** 구조 이해\n",
    "2. **다층 퍼셉트론(MLPs: Multi-Layer Perceptrons)** 을 TensorFlow로 구현하고 MNIST 분류 문제를 풀어보자"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Artifcial Neural Netorks(ANNs)\n",
    "> - 기원은 뇌의 구조\n",
    "> - 자연을 본따는 인간의 습성이 그러하듯 \n",
    "> - 하지만 창의성을 위해서는 생물학적 사고 안에서 머물러서는 안된다.\n",
    "> - ANNs는 Deep Learning의 핵심이다.\n",
    "> - 다재다능/강력/확장성으로 어려운 머신러닝 숙제를 잘 풀어간다. (이미지 분류, 음성인식, 추천시스템, 게임)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From Biological to Artificial Neurons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ANNs 는 출현한지 좀 오래된 알고리즘\n",
    " - 1943 신경생리학자 Warren McCulloch와 수학자 Walter Pitts \"A Logical Calculus of Ideas Immanent in Nervous Activity\"\n",
    "   - **논리연산을 사용한 복잡한 계산을 수행시, 동물 두뇌에서 생물학적 뉴런이 작동하는 단순화된 계산 모델 제시** ← 최초 ann 구조\n",
    " - 이후 다른 구조들이 발명되었음\n",
    "\n",
    "\n",
    "- 1960년대까지 ANN 초기 성공은 지능이 있는 기계와 대화할거라는 믿음을 이끌어냈다.\n",
    "- 80년대 초반까지는 암흑기(펀딩x), 새로운 구조와 학습 기술이 개발되면서 다시 관심이 생겨남\n",
    "- 90년대에는 SVM에 밀림(결과가 좋고 이론적 기초가 탄탄)\n",
    "- 최근에는...\n",
    " - <font color='red'>데이터</font>가 방대함, 다른 ML 테크닉보다 크고 복잡한 문제에 성능이 더 좋음\n",
    " - 1990 년대 이후로 <font color='red'>컴퓨팅 파워(GPU, 게임 업계 덕분)</font>가 엄청나게 개선된 덕분에 합리적인 시간에 대규모 신경 네트워크를 학습 할 수 있음\n",
    " - <font color='red'>학습 알고리즘이 개선</font>되었습니다. 사실 그들은 1990 년대에 사용 된 것들과 약간만 다르지만, 이런 작은 비틀기는 엄청난 긍정적 영향을 미침\n",
    " - ANN의 이론적 한계중 일부가 현업에서 쓰기에 무리가 없음이 밝혀짐\n",
    "   - **ex) ANN 교육 알고리즘이 Local 최적 상태에 머물러 있기 때문에 한계에 부딪혔다 고 생각하지만 실제로는 현업에서 그럴일이 거의 없음 \n",
    "   (또는 실제로 그런 경우 일반적으로 Global 최적에 가까움).**\n",
    " - ANN은 자금 조달 및 진보의 선순환 들어갔다. 헤드라인 뉴스 제조기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Biological Neurons\n",
    "> 고차원의 복잡한 계산이 단순한 뉴론들의 Multi layers로 수행될 수 있다. Biological 뉴런 네트워크의 The architecture은 여전히 핫한 연구 주제"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logical Computations with Neurons\n",
    "(assuming that a neuron is activated when at least two of its inputs are active)\n",
    " <img src=\"images/book_img/ch10/10_p1.SimpleANN.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 가장 간단한 ANN 구조 by 1957 Frank Rosenblatt\n",
    " - linear threshold unit(LTU) : 이진 분류에 사용 (just like a Logistic Regression classifier or a linear SVM) \n",
    "   - input&ouput: numbers (not binary on/off values)\n",
    "   - 각 입력연결은 가중치와 연결된다.\n",
    "   - 입력갑들의 합들을 계산하고, 그 합을 임계값을 경계로 출력하는 2단계로 처리\n",
    "   \n",
    "\n",
    "- Perceptron은 단일 LTU로 구성되어 있다.\n",
    " - input neurons\n",
    " - bias neuron\n",
    "   \n",
    "`\"퍼셉트론에서는 활성화 함수로 계단 함수를 이용한다\"`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/book_img/ch10/10_p2.LTU.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/book_img/ch10/10_p2_2.perceptron_diagram.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Heaviside Step Function(계단함수) 과 부호함수(sign function)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/book_img/ch10/10_p3.StepFunction.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Percpetron 학습 방법\n",
    " - training algorithm은 Hebb's rule에 영감을 받은 Frank Rosenblatt 에 의해 제안됨\n",
    "   - \"어떤 신경세포의 활성이 다른 신경세포가 활성하는데 계속적으로 공헌을 한다면, 두 신경세포 간의 연결 가중치를 증가시켜 주어야 한다.\"\n",
    "   __ The Organization of Behavior, 1949, Donald Hebb ☆"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/book_img/ch10/10_p4.Perceptron learning.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - 각 ouput neuron 의 decision boundary 가 선형이므로, perceptron은 복잡한 패턴을 학습하는데 한계가 있다. (Logistic Regression calssifier 처럼)\n",
    "- 하지만 training instances가 선형으로 분리가능하다면, 이 알고리즘이 해로 수렴함: *Percpetron convergence theorem*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data[:, (2, 3)]  # petal length, petal width\n",
    "y = (iris.target == 0).astype(np.int) # Boolean -> integer\n",
    "\n",
    "per_clf = Perceptron(random_state=42)\n",
    "per_clf.fit(X, y)\n",
    "\n",
    "y_pred = per_clf.predict([[2, 0.5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Perceptron 학습 알고리즘은 Stochastic Gradient Descent와 유사하다.\n",
    "- Scikit-Learn's Perceptron 클래스는 SGDClassifier 의 아래 하이퍼파라미터 설정으로 했을때 동일하다.\n",
    " - *loss = \"perceptron\", learning_rate=\"constant\", eta0=1 (the learning rate), penalty = None (no regularization)* ☆"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Logistic Regression classifiers과 다르게, 각 클래스의 확률을 출력하지 않고 hard threshold에 기반한 예측만 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 퍼셉트론의 약점을 기술한 Monograph(Perceptrons, Marvin Minsky and Seymour Papert, 1969)\n",
    " - Exclusive OR(XOR) 문제 못푼다.이는 다른 선형 분류기도 마찬가지만, 많은 연구자들이 connectionism 을 포기하는 계기가 되었다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 이러한 한계점은 여러 Perceptrons 들을 쌓으면서 해소될 수 있다.\n",
    "- 이 ANN은 Multi-Layer Perceptron(MLP)라 불린다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/book_img/ch10/10_p5.XOR.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Layer Perceptron and Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- MLP 는 (passthrough) input layer과, 1개 이상의 LTU (hidden layers), 마지막 ouput layer로 구성되어 있다.\n",
    " - ouput layer를 제외한 모든 레이어에 bias neuron이 있고, 다음 레이어에 fully connected 되어 있다.\n",
    " - hidden layers가 2개 이상이라면 Deep nural network(DNN)라고 불린다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/book_img/ch10/10_p6.MLP.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 여러 해 동안 MLP를 학습시키기 위해 많은 연구자들이 성과 없이 고군분투 했다.\n",
    "- <font color = red>**backpropagation** 학습 알고리즘</font>을 소개하는 \"Learning Internal Represenations by Error Propagation\"(1986, D.E. Rumelhar et al) 획기적인 논문이 발간되었다.\n",
    "- 오늘날 우리가 이야기하는**reverse-mode autodiff를 이용한 Gradient Descent 이다.**\n",
    " > 개별 학습 instacne 마다... 모든 누적 레이어에 대해 feed forward pass 이후에 ouput error를 보고, 마지막 hidden layer의 각 뉴런이 ouput error에 얼마나 기여를 했는지 계산한다. 후에 input layer까지 각 이전 hidden layer에서 에러 기여도를 측정하는 방법\n",
    "   - 이러한 reverse pass(**<font color=red>error gradient</font>를 네트워크에서 역으로 전파**함으로써)는 네트워크의 모든 연결된 weight에 대해 error gradient를 효과적으로 측정한다.\n",
    "   - 역전파(backpropatation) 알고리즘의 마지막 단계는 앞에서 측정 한 오류 기울기(error gradient)를 사용하여 네트워크의 모든 연결 가중치에 대한 Gradient Descent 을 수행한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "즉, ` \"예측을 하고(forward pass), 에러를 측정하고, 개별 layer를 역으로 추적해 나가면서 각 연결의 error contribution를 계산한다(reverse pass). 그리고 에러를 줄이는 방향으로 가중치를 조정한다(Gradient Descent).\" `"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 이 알고리즘이 **제대로 작동하기 위한 핵심(key change)은 step function 대신에 logistic function을 적용하는 MLP's 구조**이다.\n",
    " > logistic function이 모든 곳에서 잘정의된(0이 아닌) 미분계수를 구해주고, 이를 통해 Gradient Descent를 각 단계마다 성공적으로 진전시킬수 있다.\n",
    " > logistic function 대신에 다른 activation function을 적용할수도 있다.\n",
    "   - *The hyperbolic tangent function*\n",
    "    : 1~1(logistic functoin: 0 ~ 1) / normalized(helps speed up convergence)\n",
    "   - *The ReLU function*\n",
    "     : ReLU (z) = max(0, z): 연속적이지만 미분가능하지 않다.(기울기가 급격히 변해서 경사하강이 잘 안된다) / 실제로는 계산이 빠른 이점이 있다. 가장 중요한 것은 출력 값이 최대치가 아니기 때문에 경사하강 중 일부 문제를 줄일 수 있다는 것입니다. ☆\n",
    "   \n",
    "  - 생물학적 뉴런은 활성화 함수를 시그모이드(S-자) 를 구현한다. 따라서 이를 오랫동안 고수했지만, ANN에서 ReLU가 더 잘 작동하는 것이 발견되었다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logit(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def relu(z):\n",
    "    return np.maximum(0, z)\n",
    "\n",
    "def derivative(f, z, eps=0.000001):\n",
    "    return (f(z + eps) - f(z - eps))/(2 * eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-5, 5, -0.2, 1.2]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAApsAAAEMCAYAAACLGX0CAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x109689198>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "z = np.linspace(-5, 5, 200)\n",
    "\n",
    "plt.figure(figsize=(11,4))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.plot(z, np.sign(z), \"r-\", linewidth=2, label=\"Step\")\n",
    "plt.plot(z, logit(z), \"g--\", linewidth=2, label=\"Logit\")\n",
    "plt.plot(z, np.tanh(z), \"b-\", linewidth=2, label=\"Tanh\")\n",
    "plt.plot(z, relu(z), \"m-.\", linewidth=2, label=\"ReLU\")\n",
    "plt.grid(True)\n",
    "plt.legend(loc=\"center right\", fontsize=14)\n",
    "plt.title(\"Activation functions\", fontsize=14)\n",
    "plt.axis([-5, 5, -1.2, 1.2])\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(z, derivative(np.sign, z), \"r-\", linewidth=2, label=\"Step\")\n",
    "plt.plot(0, 0, \"ro\", markersize=5)\n",
    "plt.plot(0, 0, \"rx\", markersize=10)\n",
    "plt.plot(z, derivative(logit, z), \"g--\", linewidth=2, label=\"Logit\")\n",
    "plt.plot(z, derivative(np.tanh, z), \"b-\", linewidth=2, label=\"Tanh\")\n",
    "plt.plot(z, derivative(relu, z), \"m-.\", linewidth=2, label=\"ReLU\")\n",
    "plt.grid(True)\n",
    "#plt.legend(loc=\"center right\", fontsize=14)\n",
    "plt.title(\"Derivatives\", fontsize=14)\n",
    "plt.axis([-5, 5, -0.2, 1.2])\n",
    "\n",
    "# save_fig(\"activation_functions_plot\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- MLP는 이진 분류기(스팸, 긴급 알림 등)에 자주 쓰인다.\n",
    "- 결과가 배타적이라면(이지 분류가 0~9까지 잇을때 등) output layer를 individual activation function 이 아닌 shared soft-max 함수를 적용하면 된다.\n",
    " - 각 뉴런의 출력은 해당 클래스의 예상 확률에 해당한다.\n",
    "\n",
    "Note that the signal flows only in one direction (from the inputs to the outputs), so this architecture is an example of a feedforward neural network ☆"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/book_img/ch10/10_p7.MLP_wSpftmax.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def heaviside(z):\n",
    "    return (z >= 0).astype(z.dtype)\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1/(1+np.exp(-z))\n",
    "\n",
    "def mlp_xor(x1, x2, activation=heaviside):\n",
    "    return activation(-activation(x1 + x2 - 1.5) + activation(x1 + x2 - 0.5) - 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmkAAAEMCAYAAACfoCGmAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\npImIiIgUkEKaiIiISAEppImIiIgU0P8Hut8DXs9ag3AAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x109592be0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x1s = np.linspace(-0.2, 1.2, 100)\n",
    "x2s = np.linspace(-0.2, 1.2, 100)\n",
    "x1, x2 = np.meshgrid(x1s, x2s)\n",
    "\n",
    "z1 = mlp_xor(x1, x2, activation=heaviside)\n",
    "z2 = mlp_xor(x1, x2, activation=sigmoid)\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.contourf(x1, x2, z1)\n",
    "plt.plot([0, 1], [0, 1], \"gs\", markersize=20)\n",
    "plt.plot([0, 1], [1, 0], \"y^\", markersize=20)\n",
    "plt.title(\"Activation function: heaviside\", fontsize=14)\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.contourf(x1, x2, z2)\n",
    "plt.plot([0, 1], [0, 1], \"gs\", markersize=20)\n",
    "plt.plot([0, 1], [1, 0], \"y^\", markersize=20)\n",
    "plt.title(\"Activation function: sigmoid\", fontsize=14)\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training an MLP with TensorFlow's High-Level API\n",
    "## using tf.learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- TF.Learn의 DNN Classifier 클래스는 hidden layer 와 softmax output layer을 쉽게 만들어준다.\n",
    "- (예제) DNN 2개 hiedden layers(각각 300개, 100개 뉴런)과 1개의 output layer (10개 뉴런) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = mnist.train.images\n",
    "X_test = mnist.test.images\n",
    "y_train = mnist.train.labels.astype(\"int\")\n",
    "y_test = mnist.test.labels.astype(\"int\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_task_type': None, '_task_id': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x131f1e7b8>, '_master': '', '_num_ps_replicas': 0, '_num_worker_replicas': 0, '_environment': 'local', '_is_chief': True, '_evaluation_master': '', '_tf_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 1\n",
      "}\n",
      ", '_tf_random_seed': 42, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_save_checkpoints_steps': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_model_dir': None}\n",
      "WARNING:tensorflow:Using temporary folder as model directory: /var/folders/rz/pq7kq6qn5k757n49mq6f4gth0000gn/T/tmpateyigox\n",
      "WARNING:tensorflow:From /Users/qbinson/.virtualenvs/elice/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/estimators/head.py:615: scalar_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.scalar. Note that tf.summary.scalar uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, passing a tensor or list of tags to a scalar summary op is no longer supported.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into /var/folders/rz/pq7kq6qn5k757n49mq6f4gth0000gn/T/tmpateyigox/model.ckpt.\n",
      "INFO:tensorflow:loss = 2.36511, step = 1\n",
      "INFO:tensorflow:global_step/sec: 155.652\n",
      "INFO:tensorflow:loss = 0.352858, step = 101 (0.644 sec)\n",
      "INFO:tensorflow:global_step/sec: 193.149\n",
      "INFO:tensorflow:loss = 0.257753, step = 201 (0.518 sec)\n",
      "INFO:tensorflow:global_step/sec: 193.083\n",
      "INFO:tensorflow:loss = 0.374227, step = 301 (0.518 sec)\n",
      "INFO:tensorflow:global_step/sec: 192.857\n",
      "INFO:tensorflow:loss = 0.242888, step = 401 (0.519 sec)\n",
      "INFO:tensorflow:global_step/sec: 189.179\n",
      "INFO:tensorflow:loss = 0.265575, step = 501 (0.529 sec)\n",
      "INFO:tensorflow:global_step/sec: 193.113\n",
      "INFO:tensorflow:loss = 0.0870075, step = 601 (0.518 sec)\n",
      "INFO:tensorflow:global_step/sec: 193.252\n",
      "INFO:tensorflow:loss = 0.118208, step = 701 (0.518 sec)\n",
      "INFO:tensorflow:global_step/sec: 194.27\n",
      "INFO:tensorflow:loss = 0.196863, step = 801 (0.515 sec)\n",
      "INFO:tensorflow:global_step/sec: 188.953\n",
      "INFO:tensorflow:loss = 0.113669, step = 901 (0.529 sec)\n",
      "INFO:tensorflow:global_step/sec: 193.188\n",
      "INFO:tensorflow:loss = 0.232583, step = 1001 (0.518 sec)\n",
      "INFO:tensorflow:global_step/sec: 193.868\n",
      "INFO:tensorflow:loss = 0.173962, step = 1101 (0.516 sec)\n",
      "INFO:tensorflow:global_step/sec: 193.092\n",
      "INFO:tensorflow:loss = 0.12305, step = 1201 (0.518 sec)\n",
      "INFO:tensorflow:global_step/sec: 192.154\n",
      "INFO:tensorflow:loss = 0.156566, step = 1301 (0.521 sec)\n",
      "INFO:tensorflow:global_step/sec: 191.838\n",
      "INFO:tensorflow:loss = 0.0733498, step = 1401 (0.521 sec)\n",
      "INFO:tensorflow:global_step/sec: 190.841\n",
      "INFO:tensorflow:loss = 0.11247, step = 1501 (0.524 sec)\n",
      "INFO:tensorflow:global_step/sec: 193.084\n",
      "INFO:tensorflow:loss = 0.156935, step = 1601 (0.518 sec)\n",
      "INFO:tensorflow:global_step/sec: 190.79\n",
      "INFO:tensorflow:loss = 0.0358028, step = 1701 (0.524 sec)\n",
      "INFO:tensorflow:global_step/sec: 192.123\n",
      "INFO:tensorflow:loss = 0.179867, step = 1801 (0.520 sec)\n",
      "INFO:tensorflow:global_step/sec: 192.604\n",
      "INFO:tensorflow:loss = 0.133954, step = 1901 (0.519 sec)\n",
      "INFO:tensorflow:global_step/sec: 192.284\n",
      "INFO:tensorflow:loss = 0.131093, step = 2001 (0.520 sec)\n",
      "INFO:tensorflow:global_step/sec: 193.873\n",
      "INFO:tensorflow:loss = 0.0225927, step = 2101 (0.516 sec)\n",
      "INFO:tensorflow:global_step/sec: 189.626\n",
      "INFO:tensorflow:loss = 0.0311291, step = 2201 (0.527 sec)\n",
      "INFO:tensorflow:global_step/sec: 195.271\n",
      "INFO:tensorflow:loss = 0.0653558, step = 2301 (0.512 sec)\n",
      "INFO:tensorflow:global_step/sec: 192.389\n",
      "INFO:tensorflow:loss = 0.0513635, step = 2401 (0.520 sec)\n",
      "INFO:tensorflow:global_step/sec: 190.923\n",
      "INFO:tensorflow:loss = 0.108849, step = 2501 (0.524 sec)\n",
      "INFO:tensorflow:global_step/sec: 192.062\n",
      "INFO:tensorflow:loss = 0.0525253, step = 2601 (0.520 sec)\n",
      "INFO:tensorflow:global_step/sec: 189.369\n",
      "INFO:tensorflow:loss = 0.0108695, step = 2701 (0.528 sec)\n",
      "INFO:tensorflow:global_step/sec: 194.183\n",
      "INFO:tensorflow:loss = 0.0373951, step = 2801 (0.515 sec)\n",
      "INFO:tensorflow:global_step/sec: 192.142\n",
      "INFO:tensorflow:loss = 0.122921, step = 2901 (0.521 sec)\n",
      "INFO:tensorflow:global_step/sec: 191.511\n",
      "INFO:tensorflow:loss = 0.0181274, step = 3001 (0.522 sec)\n",
      "INFO:tensorflow:global_step/sec: 193.282\n",
      "INFO:tensorflow:loss = 0.0449047, step = 3101 (0.517 sec)\n",
      "INFO:tensorflow:global_step/sec: 194.764\n",
      "INFO:tensorflow:loss = 0.0176369, step = 3201 (0.513 sec)\n",
      "INFO:tensorflow:global_step/sec: 193.613\n",
      "INFO:tensorflow:loss = 0.0176865, step = 3301 (0.517 sec)\n",
      "INFO:tensorflow:global_step/sec: 193.678\n",
      "INFO:tensorflow:loss = 0.213318, step = 3401 (0.516 sec)\n",
      "INFO:tensorflow:global_step/sec: 190.704\n",
      "INFO:tensorflow:loss = 0.0650596, step = 3501 (0.524 sec)\n",
      "INFO:tensorflow:global_step/sec: 193.245\n",
      "INFO:tensorflow:loss = 0.167223, step = 3601 (0.518 sec)\n",
      "INFO:tensorflow:global_step/sec: 191.904\n",
      "INFO:tensorflow:loss = 0.035286, step = 3701 (0.521 sec)\n",
      "INFO:tensorflow:global_step/sec: 193.014\n",
      "INFO:tensorflow:loss = 0.0153058, step = 3801 (0.518 sec)\n",
      "INFO:tensorflow:global_step/sec: 192.704\n",
      "INFO:tensorflow:loss = 0.121445, step = 3901 (0.519 sec)\n",
      "INFO:tensorflow:global_step/sec: 193.465\n",
      "INFO:tensorflow:loss = 0.103279, step = 4001 (0.517 sec)\n",
      "INFO:tensorflow:global_step/sec: 191.84\n",
      "INFO:tensorflow:loss = 0.0329011, step = 4101 (0.521 sec)\n",
      "INFO:tensorflow:global_step/sec: 193.645\n",
      "INFO:tensorflow:loss = 0.0429299, step = 4201 (0.516 sec)\n",
      "INFO:tensorflow:global_step/sec: 192.64\n",
      "INFO:tensorflow:loss = 0.191253, step = 4301 (0.519 sec)\n",
      "INFO:tensorflow:global_step/sec: 191.716\n",
      "INFO:tensorflow:loss = 0.115447, step = 4401 (0.522 sec)\n",
      "INFO:tensorflow:global_step/sec: 201.911\n",
      "INFO:tensorflow:loss = 0.0185967, step = 4501 (0.495 sec)\n",
      "INFO:tensorflow:global_step/sec: 200.27\n",
      "INFO:tensorflow:loss = 0.023272, step = 4601 (0.499 sec)\n",
      "INFO:tensorflow:global_step/sec: 203.309\n",
      "INFO:tensorflow:loss = 0.008589, step = 4701 (0.492 sec)\n",
      "INFO:tensorflow:global_step/sec: 200.306\n",
      "INFO:tensorflow:loss = 0.0238874, step = 4801 (0.499 sec)\n",
      "INFO:tensorflow:global_step/sec: 202.199\n",
      "INFO:tensorflow:loss = 0.0644178, step = 4901 (0.495 sec)\n",
      "INFO:tensorflow:global_step/sec: 201.383\n",
      "INFO:tensorflow:loss = 0.126686, step = 5001 (0.497 sec)\n",
      "INFO:tensorflow:global_step/sec: 200.056\n",
      "INFO:tensorflow:loss = 0.015281, step = 5101 (0.500 sec)\n",
      "INFO:tensorflow:global_step/sec: 199.363\n",
      "INFO:tensorflow:loss = 0.0407446, step = 5201 (0.502 sec)\n",
      "INFO:tensorflow:global_step/sec: 201.566\n",
      "INFO:tensorflow:loss = 0.0255002, step = 5301 (0.496 sec)\n",
      "INFO:tensorflow:global_step/sec: 202.294\n",
      "INFO:tensorflow:loss = 0.0481827, step = 5401 (0.494 sec)\n",
      "INFO:tensorflow:global_step/sec: 196.942\n",
      "INFO:tensorflow:loss = 0.0239235, step = 5501 (0.508 sec)\n",
      "INFO:tensorflow:global_step/sec: 198.16\n",
      "INFO:tensorflow:loss = 0.116352, step = 5601 (0.505 sec)\n",
      "INFO:tensorflow:global_step/sec: 197.466\n",
      "INFO:tensorflow:loss = 0.0159615, step = 5701 (0.506 sec)\n",
      "INFO:tensorflow:global_step/sec: 202.702\n",
      "INFO:tensorflow:loss = 0.0104831, step = 5801 (0.493 sec)\n",
      "INFO:tensorflow:global_step/sec: 202.497\n",
      "INFO:tensorflow:loss = 0.0832038, step = 5901 (0.494 sec)\n",
      "INFO:tensorflow:global_step/sec: 202.22\n",
      "INFO:tensorflow:loss = 0.110211, step = 6001 (0.495 sec)\n",
      "INFO:tensorflow:global_step/sec: 197.755\n",
      "INFO:tensorflow:loss = 0.02048, step = 6101 (0.506 sec)\n",
      "INFO:tensorflow:global_step/sec: 203.187\n",
      "INFO:tensorflow:loss = 0.0197042, step = 6201 (0.492 sec)\n",
      "INFO:tensorflow:global_step/sec: 200.923\n",
      "INFO:tensorflow:loss = 0.0647402, step = 6301 (0.498 sec)\n",
      "INFO:tensorflow:global_step/sec: 197.229\n",
      "INFO:tensorflow:loss = 0.0166577, step = 6401 (0.507 sec)\n",
      "INFO:tensorflow:global_step/sec: 201.511\n",
      "INFO:tensorflow:loss = 0.0120903, step = 6501 (0.496 sec)\n",
      "INFO:tensorflow:global_step/sec: 198.866\n",
      "INFO:tensorflow:loss = 0.0133222, step = 6601 (0.503 sec)\n",
      "INFO:tensorflow:global_step/sec: 198.814\n",
      "INFO:tensorflow:loss = 0.0151751, step = 6701 (0.503 sec)\n",
      "INFO:tensorflow:global_step/sec: 202.955\n",
      "INFO:tensorflow:loss = 0.0065389, step = 6801 (0.493 sec)\n",
      "INFO:tensorflow:global_step/sec: 201.219\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.0136699, step = 6901 (0.497 sec)\n",
      "INFO:tensorflow:global_step/sec: 202.245\n",
      "INFO:tensorflow:loss = 0.0266123, step = 7001 (0.495 sec)\n",
      "INFO:tensorflow:global_step/sec: 199.101\n",
      "INFO:tensorflow:loss = 0.00661904, step = 7101 (0.502 sec)\n",
      "INFO:tensorflow:global_step/sec: 202.827\n",
      "INFO:tensorflow:loss = 0.0382335, step = 7201 (0.493 sec)\n",
      "INFO:tensorflow:global_step/sec: 202.557\n",
      "INFO:tensorflow:loss = 0.00938992, step = 7301 (0.494 sec)\n",
      "INFO:tensorflow:global_step/sec: 203.639\n",
      "INFO:tensorflow:loss = 0.0176395, step = 7401 (0.491 sec)\n",
      "INFO:tensorflow:global_step/sec: 195.016\n",
      "INFO:tensorflow:loss = 0.00269845, step = 7501 (0.513 sec)\n",
      "INFO:tensorflow:global_step/sec: 193.946\n",
      "INFO:tensorflow:loss = 0.0172509, step = 7601 (0.515 sec)\n",
      "INFO:tensorflow:global_step/sec: 191.295\n",
      "INFO:tensorflow:loss = 0.00714039, step = 7701 (0.523 sec)\n",
      "INFO:tensorflow:global_step/sec: 142.613\n",
      "INFO:tensorflow:loss = 0.00676612, step = 7801 (0.702 sec)\n",
      "INFO:tensorflow:global_step/sec: 166.55\n",
      "INFO:tensorflow:loss = 0.0104606, step = 7901 (0.600 sec)\n",
      "INFO:tensorflow:global_step/sec: 175.432\n",
      "INFO:tensorflow:loss = 0.00164501, step = 8001 (0.570 sec)\n",
      "INFO:tensorflow:global_step/sec: 191.358\n",
      "INFO:tensorflow:loss = 0.0281608, step = 8101 (0.522 sec)\n",
      "INFO:tensorflow:global_step/sec: 206.372\n",
      "INFO:tensorflow:loss = 0.0116268, step = 8201 (0.485 sec)\n",
      "INFO:tensorflow:global_step/sec: 213.055\n",
      "INFO:tensorflow:loss = 0.0343281, step = 8301 (0.469 sec)\n",
      "INFO:tensorflow:global_step/sec: 210.236\n",
      "INFO:tensorflow:loss = 0.00815744, step = 8401 (0.476 sec)\n",
      "INFO:tensorflow:global_step/sec: 213.981\n",
      "INFO:tensorflow:loss = 0.00329395, step = 8501 (0.467 sec)\n",
      "INFO:tensorflow:global_step/sec: 212.153\n",
      "INFO:tensorflow:loss = 0.00260592, step = 8601 (0.471 sec)\n",
      "INFO:tensorflow:global_step/sec: 211.137\n",
      "INFO:tensorflow:loss = 0.0017944, step = 8701 (0.474 sec)\n",
      "INFO:tensorflow:global_step/sec: 209.491\n",
      "INFO:tensorflow:loss = 0.0274318, step = 8801 (0.477 sec)\n",
      "INFO:tensorflow:global_step/sec: 210.502\n",
      "INFO:tensorflow:loss = 0.00224499, step = 8901 (0.475 sec)\n",
      "INFO:tensorflow:global_step/sec: 206.507\n",
      "INFO:tensorflow:loss = 0.00663088, step = 9001 (0.484 sec)\n",
      "INFO:tensorflow:global_step/sec: 192.48\n",
      "INFO:tensorflow:loss = 0.00479007, step = 9101 (0.520 sec)\n",
      "INFO:tensorflow:global_step/sec: 206.087\n",
      "INFO:tensorflow:loss = 0.00719844, step = 9201 (0.485 sec)\n",
      "INFO:tensorflow:global_step/sec: 231.449\n",
      "INFO:tensorflow:loss = 0.00686181, step = 9301 (0.432 sec)\n",
      "INFO:tensorflow:global_step/sec: 195.967\n",
      "INFO:tensorflow:loss = 0.0561124, step = 9401 (0.510 sec)\n",
      "INFO:tensorflow:global_step/sec: 230.696\n",
      "INFO:tensorflow:loss = 0.00679949, step = 9501 (0.433 sec)\n",
      "INFO:tensorflow:global_step/sec: 223.626\n",
      "INFO:tensorflow:loss = 0.0036581, step = 9601 (0.447 sec)\n",
      "INFO:tensorflow:global_step/sec: 221.475\n",
      "INFO:tensorflow:loss = 0.00430781, step = 9701 (0.451 sec)\n",
      "INFO:tensorflow:global_step/sec: 224.658\n",
      "INFO:tensorflow:loss = 0.00157378, step = 9801 (0.445 sec)\n",
      "INFO:tensorflow:global_step/sec: 215.601\n",
      "INFO:tensorflow:loss = 0.0152471, step = 9901 (0.464 sec)\n",
      "INFO:tensorflow:global_step/sec: 218.959\n",
      "INFO:tensorflow:loss = 0.0197686, step = 10001 (0.457 sec)\n",
      "INFO:tensorflow:global_step/sec: 218.386\n",
      "INFO:tensorflow:loss = 0.00875743, step = 10101 (0.458 sec)\n",
      "INFO:tensorflow:global_step/sec: 218.855\n",
      "INFO:tensorflow:loss = 0.0103044, step = 10201 (0.457 sec)\n",
      "INFO:tensorflow:global_step/sec: 216.844\n",
      "INFO:tensorflow:loss = 0.00320183, step = 10301 (0.461 sec)\n",
      "INFO:tensorflow:global_step/sec: 220.141\n",
      "INFO:tensorflow:loss = 0.00661283, step = 10401 (0.454 sec)\n",
      "INFO:tensorflow:global_step/sec: 215.674\n",
      "INFO:tensorflow:loss = 0.00155472, step = 10501 (0.464 sec)\n",
      "INFO:tensorflow:global_step/sec: 211.175\n",
      "INFO:tensorflow:loss = 0.00894439, step = 10601 (0.473 sec)\n",
      "INFO:tensorflow:global_step/sec: 217.955\n",
      "INFO:tensorflow:loss = 0.0254303, step = 10701 (0.459 sec)\n",
      "INFO:tensorflow:global_step/sec: 219.134\n",
      "INFO:tensorflow:loss = 0.00407793, step = 10801 (0.456 sec)\n",
      "INFO:tensorflow:global_step/sec: 215.501\n",
      "INFO:tensorflow:loss = 0.00499504, step = 10901 (0.464 sec)\n",
      "INFO:tensorflow:global_step/sec: 212.718\n",
      "INFO:tensorflow:loss = 0.0164442, step = 11001 (0.470 sec)\n",
      "INFO:tensorflow:global_step/sec: 220.929\n",
      "INFO:tensorflow:loss = 0.00307142, step = 11101 (0.452 sec)\n",
      "INFO:tensorflow:global_step/sec: 221.117\n",
      "INFO:tensorflow:loss = 0.00108776, step = 11201 (0.452 sec)\n",
      "INFO:tensorflow:global_step/sec: 218.251\n",
      "INFO:tensorflow:loss = 0.00909141, step = 11301 (0.458 sec)\n",
      "INFO:tensorflow:global_step/sec: 208.968\n",
      "INFO:tensorflow:loss = 0.0054372, step = 11401 (0.479 sec)\n",
      "INFO:tensorflow:global_step/sec: 214.432\n",
      "INFO:tensorflow:loss = 0.00922716, step = 11501 (0.466 sec)\n",
      "INFO:tensorflow:global_step/sec: 209.203\n",
      "INFO:tensorflow:loss = 0.000940371, step = 11601 (0.478 sec)\n",
      "INFO:tensorflow:global_step/sec: 207.732\n",
      "INFO:tensorflow:loss = 0.00358525, step = 11701 (0.481 sec)\n",
      "INFO:tensorflow:global_step/sec: 203.335\n",
      "INFO:tensorflow:loss = 0.000163179, step = 11801 (0.493 sec)\n",
      "INFO:tensorflow:global_step/sec: 204.754\n",
      "INFO:tensorflow:loss = 0.00348126, step = 11901 (0.487 sec)\n",
      "INFO:tensorflow:global_step/sec: 206\n",
      "INFO:tensorflow:loss = 0.000426022, step = 12001 (0.486 sec)\n",
      "INFO:tensorflow:global_step/sec: 208.803\n",
      "INFO:tensorflow:loss = 0.00667039, step = 12101 (0.479 sec)\n",
      "INFO:tensorflow:global_step/sec: 207.164\n",
      "INFO:tensorflow:loss = 0.0111469, step = 12201 (0.483 sec)\n",
      "INFO:tensorflow:global_step/sec: 212.849\n",
      "INFO:tensorflow:loss = 0.00573084, step = 12301 (0.470 sec)\n",
      "INFO:tensorflow:global_step/sec: 207.898\n",
      "INFO:tensorflow:loss = 0.000534757, step = 12401 (0.481 sec)\n",
      "INFO:tensorflow:global_step/sec: 207.442\n",
      "INFO:tensorflow:loss = 0.00218082, step = 12501 (0.482 sec)\n",
      "INFO:tensorflow:global_step/sec: 206.502\n",
      "INFO:tensorflow:loss = 0.00126505, step = 12601 (0.484 sec)\n",
      "INFO:tensorflow:global_step/sec: 205.389\n",
      "INFO:tensorflow:loss = 0.00439872, step = 12701 (0.487 sec)\n",
      "INFO:tensorflow:global_step/sec: 208.746\n",
      "INFO:tensorflow:loss = 0.00857892, step = 12801 (0.479 sec)\n",
      "INFO:tensorflow:global_step/sec: 207.067\n",
      "INFO:tensorflow:loss = 0.00320482, step = 12901 (0.482 sec)\n",
      "INFO:tensorflow:global_step/sec: 207.567\n",
      "INFO:tensorflow:loss = 0.00480689, step = 13001 (0.482 sec)\n",
      "INFO:tensorflow:global_step/sec: 202.689\n",
      "INFO:tensorflow:loss = 0.00174232, step = 13101 (0.493 sec)\n",
      "INFO:tensorflow:global_step/sec: 203.196\n",
      "INFO:tensorflow:loss = 0.00201284, step = 13201 (0.492 sec)\n",
      "INFO:tensorflow:global_step/sec: 208.368\n",
      "INFO:tensorflow:loss = 0.00452757, step = 13301 (0.480 sec)\n",
      "INFO:tensorflow:global_step/sec: 205.643\n",
      "INFO:tensorflow:loss = 0.00745066, step = 13401 (0.486 sec)\n",
      "INFO:tensorflow:global_step/sec: 208.38\n",
      "INFO:tensorflow:loss = 0.00437639, step = 13501 (0.480 sec)\n",
      "INFO:tensorflow:global_step/sec: 207.598\n",
      "INFO:tensorflow:loss = 0.00673502, step = 13601 (0.482 sec)\n",
      "INFO:tensorflow:global_step/sec: 207.342\n",
      "INFO:tensorflow:loss = 0.00272274, step = 13701 (0.482 sec)\n",
      "INFO:tensorflow:global_step/sec: 208.332\n",
      "INFO:tensorflow:loss = 0.0042809, step = 13801 (0.480 sec)\n",
      "INFO:tensorflow:global_step/sec: 207.111\n",
      "INFO:tensorflow:loss = 0.00220351, step = 13901 (0.483 sec)\n",
      "INFO:tensorflow:global_step/sec: 206.517\n",
      "INFO:tensorflow:loss = 0.00268933, step = 14001 (0.484 sec)\n",
      "INFO:tensorflow:global_step/sec: 210.034\n",
      "INFO:tensorflow:loss = 0.0107425, step = 14101 (0.476 sec)\n",
      "INFO:tensorflow:global_step/sec: 205.345\n",
      "INFO:tensorflow:loss = 0.0026298, step = 14201 (0.487 sec)\n",
      "INFO:tensorflow:global_step/sec: 190.269\n",
      "INFO:tensorflow:loss = 0.00154992, step = 14301 (0.525 sec)\n",
      "INFO:tensorflow:global_step/sec: 206.052\n",
      "INFO:tensorflow:loss = 0.00061596, step = 14401 (0.485 sec)\n",
      "INFO:tensorflow:global_step/sec: 202.871\n",
      "INFO:tensorflow:loss = 0.000855468, step = 14501 (0.500 sec)\n",
      "INFO:tensorflow:global_step/sec: 190.872\n",
      "INFO:tensorflow:loss = 0.00425993, step = 14601 (0.516 sec)\n",
      "INFO:tensorflow:global_step/sec: 209.872\n",
      "INFO:tensorflow:loss = 0.000741548, step = 14701 (0.477 sec)\n",
      "INFO:tensorflow:global_step/sec: 206.47\n",
      "INFO:tensorflow:loss = 0.000813556, step = 14801 (0.484 sec)\n",
      "INFO:tensorflow:global_step/sec: 213.076\n",
      "INFO:tensorflow:loss = 0.00212573, step = 14901 (0.469 sec)\n",
      "INFO:tensorflow:global_step/sec: 202.476\n",
      "INFO:tensorflow:loss = 0.000602534, step = 15001 (0.494 sec)\n",
      "INFO:tensorflow:global_step/sec: 208.469\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.0021394, step = 15101 (0.480 sec)\n",
      "INFO:tensorflow:global_step/sec: 203.087\n",
      "INFO:tensorflow:loss = 0.00143217, step = 15201 (0.492 sec)\n",
      "INFO:tensorflow:global_step/sec: 199.888\n",
      "INFO:tensorflow:loss = 0.00144085, step = 15301 (0.500 sec)\n",
      "INFO:tensorflow:global_step/sec: 204.679\n",
      "INFO:tensorflow:loss = 0.00202756, step = 15401 (0.489 sec)\n",
      "INFO:tensorflow:global_step/sec: 201.838\n",
      "INFO:tensorflow:loss = 0.00536905, step = 15501 (0.495 sec)\n",
      "INFO:tensorflow:global_step/sec: 205.521\n",
      "INFO:tensorflow:loss = 0.00482624, step = 15601 (0.487 sec)\n",
      "INFO:tensorflow:global_step/sec: 205.731\n",
      "INFO:tensorflow:loss = 0.00891734, step = 15701 (0.488 sec)\n",
      "INFO:tensorflow:global_step/sec: 203.733\n",
      "INFO:tensorflow:loss = 0.000380861, step = 15801 (0.489 sec)\n",
      "INFO:tensorflow:global_step/sec: 203.752\n",
      "INFO:tensorflow:loss = 0.00026086, step = 15901 (0.491 sec)\n",
      "INFO:tensorflow:global_step/sec: 206.343\n",
      "INFO:tensorflow:loss = 0.00497545, step = 16001 (0.485 sec)\n",
      "INFO:tensorflow:global_step/sec: 206.004\n",
      "INFO:tensorflow:loss = 0.00314433, step = 16101 (0.485 sec)\n",
      "INFO:tensorflow:global_step/sec: 208.203\n",
      "INFO:tensorflow:loss = 0.000157718, step = 16201 (0.480 sec)\n",
      "INFO:tensorflow:global_step/sec: 209.065\n",
      "INFO:tensorflow:loss = 0.00320827, step = 16301 (0.478 sec)\n",
      "INFO:tensorflow:global_step/sec: 205.637\n",
      "INFO:tensorflow:loss = 0.00267011, step = 16401 (0.486 sec)\n",
      "INFO:tensorflow:global_step/sec: 209.72\n",
      "INFO:tensorflow:loss = 0.00361123, step = 16501 (0.477 sec)\n",
      "INFO:tensorflow:global_step/sec: 201.966\n",
      "INFO:tensorflow:loss = 0.00153517, step = 16601 (0.495 sec)\n",
      "INFO:tensorflow:global_step/sec: 208.712\n",
      "INFO:tensorflow:loss = 0.0019183, step = 16701 (0.479 sec)\n",
      "INFO:tensorflow:global_step/sec: 207.284\n",
      "INFO:tensorflow:loss = 0.00212518, step = 16801 (0.483 sec)\n",
      "INFO:tensorflow:global_step/sec: 209.146\n",
      "INFO:tensorflow:loss = 0.00378694, step = 16901 (0.478 sec)\n",
      "INFO:tensorflow:global_step/sec: 207.15\n",
      "INFO:tensorflow:loss = 0.00534902, step = 17001 (0.483 sec)\n",
      "INFO:tensorflow:global_step/sec: 205.33\n",
      "INFO:tensorflow:loss = 0.000740566, step = 17101 (0.487 sec)\n",
      "INFO:tensorflow:global_step/sec: 200.688\n",
      "INFO:tensorflow:loss = 0.00265377, step = 17201 (0.498 sec)\n",
      "INFO:tensorflow:global_step/sec: 205.049\n",
      "INFO:tensorflow:loss = 0.000843605, step = 17301 (0.488 sec)\n",
      "INFO:tensorflow:global_step/sec: 209.641\n",
      "INFO:tensorflow:loss = 0.00128785, step = 17401 (0.477 sec)\n",
      "INFO:tensorflow:global_step/sec: 202.09\n",
      "INFO:tensorflow:loss = 0.00156691, step = 17501 (0.495 sec)\n",
      "INFO:tensorflow:global_step/sec: 207.628\n",
      "INFO:tensorflow:loss = 0.000597074, step = 17601 (0.482 sec)\n",
      "INFO:tensorflow:global_step/sec: 206.595\n",
      "INFO:tensorflow:loss = 0.000906346, step = 17701 (0.484 sec)\n",
      "INFO:tensorflow:global_step/sec: 194.082\n",
      "INFO:tensorflow:loss = 0.000176809, step = 17801 (0.515 sec)\n",
      "INFO:tensorflow:global_step/sec: 210.053\n",
      "INFO:tensorflow:loss = 0.00352178, step = 17901 (0.476 sec)\n",
      "INFO:tensorflow:global_step/sec: 205.215\n",
      "INFO:tensorflow:loss = 0.00048411, step = 18001 (0.487 sec)\n",
      "INFO:tensorflow:global_step/sec: 204.313\n",
      "INFO:tensorflow:loss = 0.00127767, step = 18101 (0.489 sec)\n",
      "INFO:tensorflow:global_step/sec: 206.542\n",
      "INFO:tensorflow:loss = 0.00320919, step = 18201 (0.484 sec)\n",
      "INFO:tensorflow:global_step/sec: 163.86\n",
      "INFO:tensorflow:loss = 0.00878599, step = 18301 (0.611 sec)\n",
      "INFO:tensorflow:global_step/sec: 146.482\n",
      "INFO:tensorflow:loss = 0.00238536, step = 18401 (0.682 sec)\n",
      "INFO:tensorflow:global_step/sec: 206.885\n",
      "INFO:tensorflow:loss = 0.00984501, step = 18501 (0.483 sec)\n",
      "INFO:tensorflow:global_step/sec: 206.942\n",
      "INFO:tensorflow:loss = 0.00497066, step = 18601 (0.483 sec)\n",
      "INFO:tensorflow:global_step/sec: 206.145\n",
      "INFO:tensorflow:loss = 0.000668588, step = 18701 (0.485 sec)\n",
      "INFO:tensorflow:global_step/sec: 169\n",
      "INFO:tensorflow:loss = 0.00187809, step = 18801 (0.592 sec)\n",
      "INFO:tensorflow:global_step/sec: 166.799\n",
      "INFO:tensorflow:loss = 0.00183026, step = 18901 (0.600 sec)\n",
      "INFO:tensorflow:global_step/sec: 166.626\n",
      "INFO:tensorflow:loss = 0.0014901, step = 19001 (0.600 sec)\n",
      "INFO:tensorflow:global_step/sec: 144.481\n",
      "INFO:tensorflow:loss = 0.000471284, step = 19101 (0.693 sec)\n",
      "INFO:tensorflow:global_step/sec: 142.346\n",
      "INFO:tensorflow:loss = 0.000364932, step = 19201 (0.702 sec)\n",
      "INFO:tensorflow:global_step/sec: 153.254\n",
      "INFO:tensorflow:loss = 0.00652987, step = 19301 (0.652 sec)\n",
      "INFO:tensorflow:global_step/sec: 204.195\n",
      "INFO:tensorflow:loss = 0.000951108, step = 19401 (0.490 sec)\n",
      "INFO:tensorflow:global_step/sec: 219.001\n",
      "INFO:tensorflow:loss = 0.0023219, step = 19501 (0.456 sec)\n",
      "INFO:tensorflow:global_step/sec: 225.618\n",
      "INFO:tensorflow:loss = 0.000227142, step = 19601 (0.443 sec)\n",
      "INFO:tensorflow:global_step/sec: 225.021\n",
      "INFO:tensorflow:loss = 9.63949e-05, step = 19701 (0.444 sec)\n",
      "INFO:tensorflow:global_step/sec: 210.365\n",
      "INFO:tensorflow:loss = 0.000228906, step = 19801 (0.475 sec)\n",
      "INFO:tensorflow:global_step/sec: 222.832\n",
      "INFO:tensorflow:loss = 0.00216061, step = 19901 (0.449 sec)\n",
      "INFO:tensorflow:global_step/sec: 215.217\n",
      "INFO:tensorflow:loss = 0.00162188, step = 20001 (0.465 sec)\n",
      "INFO:tensorflow:global_step/sec: 209.467\n",
      "INFO:tensorflow:loss = 0.000795621, step = 20101 (0.478 sec)\n",
      "INFO:tensorflow:global_step/sec: 215.699\n",
      "INFO:tensorflow:loss = 0.00232103, step = 20201 (0.463 sec)\n",
      "INFO:tensorflow:global_step/sec: 218.831\n",
      "INFO:tensorflow:loss = 0.000946591, step = 20301 (0.457 sec)\n",
      "INFO:tensorflow:global_step/sec: 211.438\n",
      "INFO:tensorflow:loss = 0.000313094, step = 20401 (0.473 sec)\n",
      "INFO:tensorflow:global_step/sec: 216.272\n",
      "INFO:tensorflow:loss = 0.000953562, step = 20501 (0.462 sec)\n",
      "INFO:tensorflow:global_step/sec: 215.029\n",
      "INFO:tensorflow:loss = 0.000841321, step = 20601 (0.465 sec)\n",
      "INFO:tensorflow:global_step/sec: 210.458\n",
      "INFO:tensorflow:loss = 0.000420437, step = 20701 (0.475 sec)\n",
      "INFO:tensorflow:global_step/sec: 215.191\n",
      "INFO:tensorflow:loss = 0.00167637, step = 20801 (0.465 sec)\n",
      "INFO:tensorflow:global_step/sec: 215.387\n",
      "INFO:tensorflow:loss = 0.0012439, step = 20901 (0.464 sec)\n",
      "INFO:tensorflow:global_step/sec: 213.4\n",
      "INFO:tensorflow:loss = 0.00182237, step = 21001 (0.469 sec)\n",
      "INFO:tensorflow:global_step/sec: 212.047\n",
      "INFO:tensorflow:loss = 0.00108067, step = 21101 (0.471 sec)\n",
      "INFO:tensorflow:global_step/sec: 217.555\n",
      "INFO:tensorflow:loss = 0.00371119, step = 21201 (0.460 sec)\n",
      "INFO:tensorflow:global_step/sec: 216.464\n",
      "INFO:tensorflow:loss = 0.000990295, step = 21301 (0.462 sec)\n",
      "INFO:tensorflow:global_step/sec: 216.583\n",
      "INFO:tensorflow:loss = 0.00200678, step = 21401 (0.462 sec)\n",
      "INFO:tensorflow:global_step/sec: 214.723\n",
      "INFO:tensorflow:loss = 0.00011694, step = 21501 (0.466 sec)\n",
      "INFO:tensorflow:global_step/sec: 210.865\n",
      "INFO:tensorflow:loss = 0.000942511, step = 21601 (0.474 sec)\n",
      "INFO:tensorflow:global_step/sec: 215.419\n",
      "INFO:tensorflow:loss = 0.000876404, step = 21701 (0.464 sec)\n",
      "INFO:tensorflow:global_step/sec: 211.049\n",
      "INFO:tensorflow:loss = 0.000857327, step = 21801 (0.474 sec)\n",
      "INFO:tensorflow:global_step/sec: 216.239\n",
      "INFO:tensorflow:loss = 0.000496712, step = 21901 (0.463 sec)\n",
      "INFO:tensorflow:global_step/sec: 216.166\n",
      "INFO:tensorflow:loss = 1.86095e-05, step = 22001 (0.463 sec)\n",
      "INFO:tensorflow:global_step/sec: 215.916\n",
      "INFO:tensorflow:loss = 0.00026844, step = 22101 (0.463 sec)\n",
      "INFO:tensorflow:global_step/sec: 219.445\n",
      "INFO:tensorflow:loss = 0.00093183, step = 22201 (0.456 sec)\n",
      "INFO:tensorflow:global_step/sec: 211.363\n",
      "INFO:tensorflow:loss = 0.00208175, step = 22301 (0.473 sec)\n",
      "INFO:tensorflow:global_step/sec: 212.96\n",
      "INFO:tensorflow:loss = 0.00150858, step = 22401 (0.470 sec)\n",
      "INFO:tensorflow:global_step/sec: 215.538\n",
      "INFO:tensorflow:loss = 0.00122541, step = 22501 (0.464 sec)\n",
      "INFO:tensorflow:global_step/sec: 213.451\n",
      "INFO:tensorflow:loss = 0.00336533, step = 22601 (0.468 sec)\n",
      "INFO:tensorflow:global_step/sec: 210.774\n",
      "INFO:tensorflow:loss = 0.0010565, step = 22701 (0.474 sec)\n",
      "INFO:tensorflow:global_step/sec: 217.253\n",
      "INFO:tensorflow:loss = 0.00118512, step = 22801 (0.461 sec)\n",
      "INFO:tensorflow:global_step/sec: 211.724\n",
      "INFO:tensorflow:loss = 0.0027068, step = 22901 (0.472 sec)\n",
      "INFO:tensorflow:global_step/sec: 214.446\n",
      "INFO:tensorflow:loss = 0.000953251, step = 23001 (0.466 sec)\n",
      "INFO:tensorflow:global_step/sec: 215.862\n",
      "INFO:tensorflow:loss = 0.00389986, step = 23101 (0.463 sec)\n",
      "INFO:tensorflow:global_step/sec: 216.97\n",
      "INFO:tensorflow:loss = 0.00160573, step = 23201 (0.461 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 212.618\n",
      "INFO:tensorflow:loss = 0.00193569, step = 23301 (0.470 sec)\n",
      "INFO:tensorflow:global_step/sec: 215.841\n",
      "INFO:tensorflow:loss = 0.000460001, step = 23401 (0.463 sec)\n",
      "INFO:tensorflow:global_step/sec: 213.692\n",
      "INFO:tensorflow:loss = 0.000577636, step = 23501 (0.468 sec)\n",
      "INFO:tensorflow:global_step/sec: 212.73\n",
      "INFO:tensorflow:loss = 0.000578934, step = 23601 (0.470 sec)\n",
      "INFO:tensorflow:global_step/sec: 208.771\n",
      "INFO:tensorflow:loss = 0.0004731, step = 23701 (0.479 sec)\n",
      "INFO:tensorflow:global_step/sec: 216.412\n",
      "INFO:tensorflow:loss = 0.00126306, step = 23801 (0.462 sec)\n",
      "INFO:tensorflow:global_step/sec: 213.3\n",
      "INFO:tensorflow:loss = 0.00167757, step = 23901 (0.469 sec)\n",
      "INFO:tensorflow:global_step/sec: 218.115\n",
      "INFO:tensorflow:loss = 0.000704214, step = 24001 (0.458 sec)\n",
      "INFO:tensorflow:global_step/sec: 214.19\n",
      "INFO:tensorflow:loss = 0.000718476, step = 24101 (0.467 sec)\n",
      "INFO:tensorflow:global_step/sec: 216.438\n",
      "INFO:tensorflow:loss = 0.00186048, step = 24201 (0.462 sec)\n",
      "INFO:tensorflow:global_step/sec: 215.682\n",
      "INFO:tensorflow:loss = 0.000105307, step = 24301 (0.464 sec)\n",
      "INFO:tensorflow:global_step/sec: 213.752\n",
      "INFO:tensorflow:loss = 0.00153693, step = 24401 (0.468 sec)\n",
      "INFO:tensorflow:global_step/sec: 214.28\n",
      "INFO:tensorflow:loss = 0.000500249, step = 24501 (0.467 sec)\n",
      "INFO:tensorflow:global_step/sec: 213.235\n",
      "INFO:tensorflow:loss = 0.000490263, step = 24601 (0.469 sec)\n",
      "INFO:tensorflow:global_step/sec: 210.671\n",
      "INFO:tensorflow:loss = 0.000790017, step = 24701 (0.475 sec)\n",
      "INFO:tensorflow:global_step/sec: 217.92\n",
      "INFO:tensorflow:loss = 0.00110519, step = 24801 (0.459 sec)\n",
      "INFO:tensorflow:global_step/sec: 211.103\n",
      "INFO:tensorflow:loss = 0.00166808, step = 24901 (0.474 sec)\n",
      "INFO:tensorflow:global_step/sec: 214.96\n",
      "INFO:tensorflow:loss = 0.000242226, step = 25001 (0.465 sec)\n",
      "INFO:tensorflow:global_step/sec: 212.179\n",
      "INFO:tensorflow:loss = 0.000694548, step = 25101 (0.471 sec)\n",
      "INFO:tensorflow:global_step/sec: 211.309\n",
      "INFO:tensorflow:loss = 0.00198228, step = 25201 (0.473 sec)\n",
      "INFO:tensorflow:global_step/sec: 215.321\n",
      "INFO:tensorflow:loss = 0.000200497, step = 25301 (0.464 sec)\n",
      "INFO:tensorflow:global_step/sec: 214.781\n",
      "INFO:tensorflow:loss = 0.000714801, step = 25401 (0.466 sec)\n",
      "INFO:tensorflow:global_step/sec: 211.389\n",
      "INFO:tensorflow:loss = 0.000655994, step = 25501 (0.473 sec)\n",
      "INFO:tensorflow:global_step/sec: 207.709\n",
      "INFO:tensorflow:loss = 0.00118201, step = 25601 (0.481 sec)\n",
      "INFO:tensorflow:global_step/sec: 211.406\n",
      "INFO:tensorflow:loss = 0.000498771, step = 25701 (0.473 sec)\n",
      "INFO:tensorflow:global_step/sec: 208.876\n",
      "INFO:tensorflow:loss = 0.00167756, step = 25801 (0.478 sec)\n",
      "INFO:tensorflow:global_step/sec: 208.729\n",
      "INFO:tensorflow:loss = 0.0022668, step = 25901 (0.479 sec)\n",
      "INFO:tensorflow:global_step/sec: 208.091\n",
      "INFO:tensorflow:loss = 6.74691e-05, step = 26001 (0.481 sec)\n",
      "INFO:tensorflow:global_step/sec: 207.303\n",
      "INFO:tensorflow:loss = 0.000857919, step = 26101 (0.482 sec)\n",
      "INFO:tensorflow:global_step/sec: 209.166\n",
      "INFO:tensorflow:loss = 0.000844437, step = 26201 (0.478 sec)\n",
      "INFO:tensorflow:global_step/sec: 213.179\n",
      "INFO:tensorflow:loss = 0.000530821, step = 26301 (0.469 sec)\n",
      "INFO:tensorflow:global_step/sec: 206.898\n",
      "INFO:tensorflow:loss = 0.00149195, step = 26401 (0.484 sec)\n",
      "INFO:tensorflow:global_step/sec: 205.505\n",
      "INFO:tensorflow:loss = 0.00114996, step = 26501 (0.487 sec)\n",
      "INFO:tensorflow:global_step/sec: 209.286\n",
      "INFO:tensorflow:loss = 0.000703954, step = 26601 (0.478 sec)\n",
      "INFO:tensorflow:global_step/sec: 209.091\n",
      "INFO:tensorflow:loss = 0.00012149, step = 26701 (0.478 sec)\n",
      "INFO:tensorflow:global_step/sec: 209.808\n",
      "INFO:tensorflow:loss = 0.000504331, step = 26801 (0.477 sec)\n",
      "INFO:tensorflow:global_step/sec: 211.259\n",
      "INFO:tensorflow:loss = 0.000866566, step = 26901 (0.473 sec)\n",
      "INFO:tensorflow:global_step/sec: 207.58\n",
      "INFO:tensorflow:loss = 0.0007483, step = 27001 (0.482 sec)\n",
      "INFO:tensorflow:global_step/sec: 207.583\n",
      "INFO:tensorflow:loss = 0.000345829, step = 27101 (0.482 sec)\n",
      "INFO:tensorflow:global_step/sec: 210.26\n",
      "INFO:tensorflow:loss = 0.000573654, step = 27201 (0.476 sec)\n",
      "INFO:tensorflow:global_step/sec: 205.41\n",
      "INFO:tensorflow:loss = 0.00059336, step = 27301 (0.486 sec)\n",
      "INFO:tensorflow:global_step/sec: 210.888\n",
      "INFO:tensorflow:loss = 0.000194953, step = 27401 (0.475 sec)\n",
      "INFO:tensorflow:global_step/sec: 211.064\n",
      "INFO:tensorflow:loss = 0.00147546, step = 27501 (0.473 sec)\n",
      "INFO:tensorflow:global_step/sec: 210.48\n",
      "INFO:tensorflow:loss = 0.00082474, step = 27601 (0.475 sec)\n",
      "INFO:tensorflow:global_step/sec: 206.499\n",
      "INFO:tensorflow:loss = 0.000307969, step = 27701 (0.484 sec)\n",
      "INFO:tensorflow:global_step/sec: 211.292\n",
      "INFO:tensorflow:loss = 0.000197565, step = 27801 (0.473 sec)\n",
      "INFO:tensorflow:global_step/sec: 210.881\n",
      "INFO:tensorflow:loss = 0.000273013, step = 27901 (0.474 sec)\n",
      "INFO:tensorflow:global_step/sec: 205.76\n",
      "INFO:tensorflow:loss = 0.0012061, step = 28001 (0.486 sec)\n",
      "INFO:tensorflow:global_step/sec: 202.756\n",
      "INFO:tensorflow:loss = 0.000631185, step = 28101 (0.493 sec)\n",
      "INFO:tensorflow:global_step/sec: 208.786\n",
      "INFO:tensorflow:loss = 0.000930747, step = 28201 (0.479 sec)\n",
      "INFO:tensorflow:global_step/sec: 201.971\n",
      "INFO:tensorflow:loss = 0.000720547, step = 28301 (0.495 sec)\n",
      "INFO:tensorflow:global_step/sec: 210.709\n",
      "INFO:tensorflow:loss = 0.000861753, step = 28401 (0.475 sec)\n",
      "INFO:tensorflow:global_step/sec: 202.482\n",
      "INFO:tensorflow:loss = 0.000277641, step = 28501 (0.494 sec)\n",
      "INFO:tensorflow:global_step/sec: 206.504\n",
      "INFO:tensorflow:loss = 0.000260821, step = 28601 (0.484 sec)\n",
      "INFO:tensorflow:global_step/sec: 205.278\n",
      "INFO:tensorflow:loss = 0.00119995, step = 28701 (0.487 sec)\n",
      "INFO:tensorflow:global_step/sec: 212.897\n",
      "INFO:tensorflow:loss = 0.000883948, step = 28801 (0.470 sec)\n",
      "INFO:tensorflow:global_step/sec: 209.92\n",
      "INFO:tensorflow:loss = 0.000268236, step = 28901 (0.476 sec)\n",
      "INFO:tensorflow:global_step/sec: 204.547\n",
      "INFO:tensorflow:loss = 0.00113506, step = 29001 (0.489 sec)\n",
      "INFO:tensorflow:global_step/sec: 206.246\n",
      "INFO:tensorflow:loss = 0.00131327, step = 29101 (0.485 sec)\n",
      "INFO:tensorflow:global_step/sec: 207.705\n",
      "INFO:tensorflow:loss = 0.00137258, step = 29201 (0.481 sec)\n",
      "INFO:tensorflow:global_step/sec: 205.162\n",
      "INFO:tensorflow:loss = 0.00136451, step = 29301 (0.487 sec)\n",
      "INFO:tensorflow:global_step/sec: 209.283\n",
      "INFO:tensorflow:loss = 0.0011558, step = 29401 (0.478 sec)\n",
      "INFO:tensorflow:global_step/sec: 206.582\n",
      "INFO:tensorflow:loss = 0.00103526, step = 29501 (0.484 sec)\n",
      "INFO:tensorflow:global_step/sec: 203.495\n",
      "INFO:tensorflow:loss = 0.000237467, step = 29601 (0.491 sec)\n",
      "INFO:tensorflow:global_step/sec: 201.912\n",
      "INFO:tensorflow:loss = 0.000363759, step = 29701 (0.496 sec)\n",
      "INFO:tensorflow:global_step/sec: 202.445\n",
      "INFO:tensorflow:loss = 0.000700495, step = 29801 (0.494 sec)\n",
      "INFO:tensorflow:global_step/sec: 206.363\n",
      "INFO:tensorflow:loss = 0.000850763, step = 29901 (0.485 sec)\n",
      "INFO:tensorflow:global_step/sec: 208.587\n",
      "INFO:tensorflow:loss = 3.86125e-05, step = 30001 (0.479 sec)\n",
      "INFO:tensorflow:global_step/sec: 202.809\n",
      "INFO:tensorflow:loss = 0.000561409, step = 30101 (0.493 sec)\n",
      "INFO:tensorflow:global_step/sec: 203.379\n",
      "INFO:tensorflow:loss = 0.000331493, step = 30201 (0.492 sec)\n",
      "INFO:tensorflow:global_step/sec: 203.664\n",
      "INFO:tensorflow:loss = 0.00137517, step = 30301 (0.491 sec)\n",
      "INFO:tensorflow:global_step/sec: 205.134\n",
      "INFO:tensorflow:loss = 0.00105892, step = 30401 (0.487 sec)\n",
      "INFO:tensorflow:global_step/sec: 206.467\n",
      "INFO:tensorflow:loss = 0.000460833, step = 30501 (0.484 sec)\n",
      "INFO:tensorflow:global_step/sec: 208.458\n",
      "INFO:tensorflow:loss = 0.00125972, step = 30601 (0.480 sec)\n",
      "INFO:tensorflow:global_step/sec: 201.319\n",
      "INFO:tensorflow:loss = 0.000971202, step = 30701 (0.497 sec)\n",
      "INFO:tensorflow:global_step/sec: 206.206\n",
      "INFO:tensorflow:loss = 0.0013289, step = 30801 (0.485 sec)\n",
      "INFO:tensorflow:global_step/sec: 207.052\n",
      "INFO:tensorflow:loss = 0.00128003, step = 30901 (0.483 sec)\n",
      "INFO:tensorflow:global_step/sec: 207.876\n",
      "INFO:tensorflow:loss = 0.00100337, step = 31001 (0.481 sec)\n",
      "INFO:tensorflow:global_step/sec: 206.933\n",
      "INFO:tensorflow:loss = 0.00107137, step = 31101 (0.483 sec)\n",
      "INFO:tensorflow:global_step/sec: 201.304\n",
      "INFO:tensorflow:loss = 0.000367889, step = 31201 (0.497 sec)\n",
      "INFO:tensorflow:global_step/sec: 205.845\n",
      "INFO:tensorflow:loss = 0.000241854, step = 31301 (0.486 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 206.469\n",
      "INFO:tensorflow:loss = 0.000548003, step = 31401 (0.484 sec)\n",
      "INFO:tensorflow:global_step/sec: 207.133\n",
      "INFO:tensorflow:loss = 0.000182311, step = 31501 (0.483 sec)\n",
      "INFO:tensorflow:global_step/sec: 205.94\n",
      "INFO:tensorflow:loss = 0.000157721, step = 31601 (0.485 sec)\n",
      "INFO:tensorflow:global_step/sec: 202.618\n",
      "INFO:tensorflow:loss = 0.000687681, step = 31701 (0.494 sec)\n",
      "INFO:tensorflow:global_step/sec: 206.572\n",
      "INFO:tensorflow:loss = 0.000149343, step = 31801 (0.484 sec)\n",
      "INFO:tensorflow:global_step/sec: 204.137\n",
      "INFO:tensorflow:loss = 0.000716847, step = 31901 (0.490 sec)\n",
      "INFO:tensorflow:global_step/sec: 206.224\n",
      "INFO:tensorflow:loss = 0.000197099, step = 32001 (0.485 sec)\n",
      "INFO:tensorflow:global_step/sec: 208.016\n",
      "INFO:tensorflow:loss = 0.00020142, step = 32101 (0.481 sec)\n",
      "INFO:tensorflow:global_step/sec: 207.333\n",
      "INFO:tensorflow:loss = 0.000950564, step = 32201 (0.482 sec)\n",
      "INFO:tensorflow:global_step/sec: 206.616\n",
      "INFO:tensorflow:loss = 0.000478452, step = 32301 (0.484 sec)\n",
      "INFO:tensorflow:global_step/sec: 203.132\n",
      "INFO:tensorflow:loss = 0.000144719, step = 32401 (0.492 sec)\n",
      "INFO:tensorflow:global_step/sec: 203.67\n",
      "INFO:tensorflow:loss = 0.000508175, step = 32501 (0.491 sec)\n",
      "INFO:tensorflow:global_step/sec: 205.905\n",
      "INFO:tensorflow:loss = 0.000114129, step = 32601 (0.486 sec)\n",
      "INFO:tensorflow:global_step/sec: 204.758\n",
      "INFO:tensorflow:loss = 0.000920422, step = 32701 (0.488 sec)\n",
      "INFO:tensorflow:global_step/sec: 207.078\n",
      "INFO:tensorflow:loss = 0.000881146, step = 32801 (0.483 sec)\n",
      "INFO:tensorflow:global_step/sec: 204.169\n",
      "INFO:tensorflow:loss = 0.000428579, step = 32901 (0.490 sec)\n",
      "INFO:tensorflow:global_step/sec: 207.06\n",
      "INFO:tensorflow:loss = 0.000409623, step = 33001 (0.483 sec)\n",
      "INFO:tensorflow:global_step/sec: 202.114\n",
      "INFO:tensorflow:loss = 0.00189231, step = 33101 (0.495 sec)\n",
      "INFO:tensorflow:global_step/sec: 206.199\n",
      "INFO:tensorflow:loss = 0.000795905, step = 33201 (0.485 sec)\n",
      "INFO:tensorflow:global_step/sec: 204.157\n",
      "INFO:tensorflow:loss = 0.00046268, step = 33301 (0.490 sec)\n",
      "INFO:tensorflow:global_step/sec: 207.239\n",
      "INFO:tensorflow:loss = 0.00113855, step = 33401 (0.483 sec)\n",
      "INFO:tensorflow:global_step/sec: 206.962\n",
      "INFO:tensorflow:loss = 0.000152743, step = 33501 (0.483 sec)\n",
      "INFO:tensorflow:global_step/sec: 205.073\n",
      "INFO:tensorflow:loss = 0.000811236, step = 33601 (0.488 sec)\n",
      "INFO:tensorflow:global_step/sec: 208.938\n",
      "INFO:tensorflow:loss = 0.000809916, step = 33701 (0.478 sec)\n",
      "INFO:tensorflow:global_step/sec: 204.788\n",
      "INFO:tensorflow:loss = 0.000918208, step = 33801 (0.488 sec)\n",
      "INFO:tensorflow:global_step/sec: 201.252\n",
      "INFO:tensorflow:loss = 0.000935263, step = 33901 (0.497 sec)\n",
      "INFO:tensorflow:global_step/sec: 204.726\n",
      "INFO:tensorflow:loss = 0.000881325, step = 34001 (0.489 sec)\n",
      "INFO:tensorflow:global_step/sec: 203.756\n",
      "INFO:tensorflow:loss = 0.000615244, step = 34101 (0.491 sec)\n",
      "INFO:tensorflow:global_step/sec: 206.698\n",
      "INFO:tensorflow:loss = 0.00106184, step = 34201 (0.484 sec)\n",
      "INFO:tensorflow:global_step/sec: 201.774\n",
      "INFO:tensorflow:loss = 0.000245821, step = 34301 (0.495 sec)\n",
      "INFO:tensorflow:global_step/sec: 202.514\n",
      "INFO:tensorflow:loss = 0.000326162, step = 34401 (0.494 sec)\n",
      "INFO:tensorflow:global_step/sec: 207.657\n",
      "INFO:tensorflow:loss = 0.00041318, step = 34501 (0.481 sec)\n",
      "INFO:tensorflow:global_step/sec: 204.543\n",
      "INFO:tensorflow:loss = 0.000476066, step = 34601 (0.490 sec)\n",
      "INFO:tensorflow:global_step/sec: 207.564\n",
      "INFO:tensorflow:loss = 0.00120529, step = 34701 (0.481 sec)\n",
      "INFO:tensorflow:global_step/sec: 204.926\n",
      "INFO:tensorflow:loss = 0.000769799, step = 34801 (0.488 sec)\n",
      "INFO:tensorflow:global_step/sec: 202.484\n",
      "INFO:tensorflow:loss = 0.000578055, step = 34901 (0.494 sec)\n",
      "INFO:tensorflow:global_step/sec: 206.479\n",
      "INFO:tensorflow:loss = 0.000442024, step = 35001 (0.484 sec)\n",
      "INFO:tensorflow:global_step/sec: 201.469\n",
      "INFO:tensorflow:loss = 0.000854468, step = 35101 (0.497 sec)\n",
      "INFO:tensorflow:global_step/sec: 201.153\n",
      "INFO:tensorflow:loss = 0.000272656, step = 35201 (0.497 sec)\n",
      "INFO:tensorflow:global_step/sec: 149.088\n",
      "INFO:tensorflow:loss = 0.00024963, step = 35301 (0.671 sec)\n",
      "INFO:tensorflow:global_step/sec: 173.726\n",
      "INFO:tensorflow:loss = 0.00106939, step = 35401 (0.575 sec)\n",
      "INFO:tensorflow:global_step/sec: 144.766\n",
      "INFO:tensorflow:loss = 0.000302915, step = 35501 (0.691 sec)\n",
      "INFO:tensorflow:global_step/sec: 168.525\n",
      "INFO:tensorflow:loss = 0.000416483, step = 35601 (0.593 sec)\n",
      "INFO:tensorflow:global_step/sec: 204.408\n",
      "INFO:tensorflow:loss = 0.000611379, step = 35701 (0.491 sec)\n",
      "INFO:tensorflow:global_step/sec: 173.043\n",
      "INFO:tensorflow:loss = 0.000459858, step = 35801 (0.576 sec)\n",
      "INFO:tensorflow:global_step/sec: 185.064\n",
      "INFO:tensorflow:loss = 0.000544892, step = 35901 (0.540 sec)\n",
      "INFO:tensorflow:global_step/sec: 213.673\n",
      "INFO:tensorflow:loss = 0.000429645, step = 36001 (0.468 sec)\n",
      "INFO:tensorflow:global_step/sec: 173.942\n",
      "INFO:tensorflow:loss = 0.00099899, step = 36101 (0.575 sec)\n",
      "INFO:tensorflow:global_step/sec: 206.958\n",
      "INFO:tensorflow:loss = 0.000661802, step = 36201 (0.483 sec)\n",
      "INFO:tensorflow:global_step/sec: 179.952\n",
      "INFO:tensorflow:loss = 0.000226923, step = 36301 (0.555 sec)\n",
      "INFO:tensorflow:global_step/sec: 213.446\n",
      "INFO:tensorflow:loss = 0.000683845, step = 36401 (0.469 sec)\n",
      "INFO:tensorflow:global_step/sec: 179.543\n",
      "INFO:tensorflow:loss = 0.000459834, step = 36501 (0.557 sec)\n",
      "INFO:tensorflow:global_step/sec: 178.359\n",
      "INFO:tensorflow:loss = 0.000286524, step = 36601 (0.560 sec)\n",
      "INFO:tensorflow:global_step/sec: 212.286\n",
      "INFO:tensorflow:loss = 0.000347136, step = 36701 (0.471 sec)\n",
      "INFO:tensorflow:global_step/sec: 209.746\n",
      "INFO:tensorflow:loss = 0.000840433, step = 36801 (0.477 sec)\n",
      "INFO:tensorflow:global_step/sec: 211.273\n",
      "INFO:tensorflow:loss = 0.000485665, step = 36901 (0.473 sec)\n",
      "INFO:tensorflow:global_step/sec: 219.263\n",
      "INFO:tensorflow:loss = 0.000524635, step = 37001 (0.456 sec)\n",
      "INFO:tensorflow:global_step/sec: 215.363\n",
      "INFO:tensorflow:loss = 0.000240721, step = 37101 (0.464 sec)\n",
      "INFO:tensorflow:global_step/sec: 219.981\n",
      "INFO:tensorflow:loss = 0.000312367, step = 37201 (0.455 sec)\n",
      "INFO:tensorflow:global_step/sec: 216.763\n",
      "INFO:tensorflow:loss = 0.000577882, step = 37301 (0.461 sec)\n",
      "INFO:tensorflow:global_step/sec: 212.324\n",
      "INFO:tensorflow:loss = 0.00016731, step = 37401 (0.472 sec)\n",
      "INFO:tensorflow:global_step/sec: 205.023\n",
      "INFO:tensorflow:loss = 0.000101914, step = 37501 (0.488 sec)\n",
      "INFO:tensorflow:global_step/sec: 197.614\n",
      "INFO:tensorflow:loss = 0.000279575, step = 37601 (0.506 sec)\n",
      "INFO:tensorflow:global_step/sec: 217.972\n",
      "INFO:tensorflow:loss = 7.18215e-05, step = 37701 (0.460 sec)\n",
      "INFO:tensorflow:global_step/sec: 218.339\n",
      "INFO:tensorflow:loss = 0.000211501, step = 37801 (0.457 sec)\n",
      "INFO:tensorflow:global_step/sec: 227.237\n",
      "INFO:tensorflow:loss = 0.00161577, step = 37901 (0.440 sec)\n",
      "INFO:tensorflow:global_step/sec: 223.582\n",
      "INFO:tensorflow:loss = 0.000173399, step = 38001 (0.448 sec)\n",
      "INFO:tensorflow:global_step/sec: 220.853\n",
      "INFO:tensorflow:loss = 0.00112188, step = 38101 (0.453 sec)\n",
      "INFO:tensorflow:global_step/sec: 222.576\n",
      "INFO:tensorflow:loss = 0.000663832, step = 38201 (0.449 sec)\n",
      "INFO:tensorflow:global_step/sec: 223.528\n",
      "INFO:tensorflow:loss = 8.43411e-05, step = 38301 (0.447 sec)\n",
      "INFO:tensorflow:global_step/sec: 220.713\n",
      "INFO:tensorflow:loss = 0.000100784, step = 38401 (0.453 sec)\n",
      "INFO:tensorflow:global_step/sec: 221.161\n",
      "INFO:tensorflow:loss = 0.000414753, step = 38501 (0.452 sec)\n",
      "INFO:tensorflow:global_step/sec: 216.791\n",
      "INFO:tensorflow:loss = 0.000828294, step = 38601 (0.461 sec)\n",
      "INFO:tensorflow:global_step/sec: 208.672\n",
      "INFO:tensorflow:loss = 0.000358774, step = 38701 (0.479 sec)\n",
      "INFO:tensorflow:global_step/sec: 209.817\n",
      "INFO:tensorflow:loss = 0.000192998, step = 38801 (0.477 sec)\n",
      "INFO:tensorflow:global_step/sec: 216.562\n",
      "INFO:tensorflow:loss = 0.00145104, step = 38901 (0.462 sec)\n",
      "INFO:tensorflow:global_step/sec: 213.425\n",
      "INFO:tensorflow:loss = 0.0001315, step = 39001 (0.469 sec)\n",
      "INFO:tensorflow:global_step/sec: 215.085\n",
      "INFO:tensorflow:loss = 0.000921957, step = 39101 (0.465 sec)\n",
      "INFO:tensorflow:global_step/sec: 215.382\n",
      "INFO:tensorflow:loss = 0.000393361, step = 39201 (0.464 sec)\n",
      "INFO:tensorflow:global_step/sec: 215.028\n",
      "INFO:tensorflow:loss = 0.000295259, step = 39301 (0.465 sec)\n",
      "INFO:tensorflow:global_step/sec: 214.49\n",
      "INFO:tensorflow:loss = 0.000674726, step = 39401 (0.466 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 216.486\n",
      "INFO:tensorflow:loss = 0.000210663, step = 39501 (0.462 sec)\n",
      "INFO:tensorflow:global_step/sec: 213.909\n",
      "INFO:tensorflow:loss = 0.000567543, step = 39601 (0.467 sec)\n",
      "INFO:tensorflow:global_step/sec: 208.815\n",
      "INFO:tensorflow:loss = 9.82609e-05, step = 39701 (0.479 sec)\n",
      "INFO:tensorflow:global_step/sec: 211.068\n",
      "INFO:tensorflow:loss = 0.00067276, step = 39801 (0.474 sec)\n",
      "INFO:tensorflow:global_step/sec: 214.259\n",
      "INFO:tensorflow:loss = 0.000958653, step = 39901 (0.467 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 40000 into /var/folders/rz/pq7kq6qn5k757n49mq6f4gth0000gn/T/tmpateyigox/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.000424012.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SKCompat()"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "config = tf.contrib.learn.RunConfig(tf_random_seed=42) # not shown in the config\n",
    "\n",
    "feature_cols = tf.contrib.learn.infer_real_valued_columns_from_input(X_train)\n",
    "dnn_clf = tf.contrib.learn.DNNClassifier(hidden_units=[300,100], n_classes=10,\n",
    "                                         feature_columns=feature_cols, config=config)\n",
    "dnn_clf = tf.contrib.learn.SKCompat(dnn_clf) # if TensorFlow >= 1.1\n",
    "dnn_clf.fit(X_train, y_train, batch_size=50, steps=40000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /var/folders/rz/pq7kq6qn5k757n49mq6f4gth0000gn/T/tmpateyigox/model.ckpt-40000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.98250000000000004"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred = dnn_clf.predict(X_test)\n",
    "accuracy_score(y_test, y_pred['classes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.073724042319017094"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import log_loss # to evaluate\n",
    "\n",
    "y_pred_proba = y_pred['probabilities']\n",
    "log_loss(y_test, y_pred_proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a DNN Using Plain TensorFlow\n",
    "## Using plain TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Minibatch Gradient Descent 를 이용해 MNIST 데이터를 학습시켜 보자.\n",
    " - Construction phase: building the TensorFlow gratph\n",
    " - Execution phase: actually run the graph to train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construction Phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "n_inputs = 28*28  # MNIST\n",
    "n_hidden1 = 300 # num. of hidden neurons in each layer\n",
    "n_hidden2 = 100 \n",
    "n_outputs = 10 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- placeholder nodes: trainig data와 target을 나타내자\n",
    " -  batch_size, 입력 형태와 트레이닝 data의 입력 형태를 정의\n",
    " - images_placeholder = tf.placeholder(<kbd>tf.float32</kbd>, shape=(<kbd>batch_size</kbd>, <kbd>mnist.IMAGE_PIXELS</kbd>))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")# represent training data\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\") # represent target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- X의 모양은 부분적으로 만 정의된다.\n",
    "- 첫 번째 차원의 인스턴스와 두 번째 차원의 피쳐를 가진 2D 텐서 (즉, 행렬)가 될 것이며, 피쳐의 수는 28 x 28 (픽셀 당 하나의 피쳐)이 될 것이라는 것을 알고 있습니다. 하지만 각 훈련 배치에 몇 개의 인스턴스가 포함되는지는 아직 알 수 없습니다. 그래서 X의 모양은 (None, n_inputs)이다. \n",
    "- 마찬가지로 y는 인스턴스 당 하나의 엔트리를 가진 1D 텐서가되지만이 시점에서 트레이닝 배치의 크기를 알 수 없기 때문에 모양은 (없음)이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 이제 실제 신경망을 만들어 보겠습니다. 자리 표시 자 X는 입력 레이어의 역할을 한다.\n",
    "- 실행 단계에서 한 번에 하나의 학습 배치마다 대체됩니다 (학습 배치의 모든 인스턴스는 신경 네트워크에 의해 동시에 처리된다). ☆\n",
    "- 이제 두 개의 숨겨진 레이어와 출력 레이어를 만들자.\n",
    "- 두 개의 숨겨진 레이어는 거의 동일합니다. 연결되어있는 입력과 포함 된 뉴런의 수에 따라 다르다.\n",
    "- 출력 계층도 매우 유사하지만 ReLU 활성화 기능 대신 softmax 활성화 기능을 사용한다. \n",
    "- neuron_layer () 함수를 만들어서 한번에 여러개의 레이어를 만들자. 입력, 뉴런 수, 활성화 함수 및 레이어 이름을 지정하는 매개 변수가 필요하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def neuron_layer(X, n_neurons, name, activation=None):  # neuron layer를 만드는 함수\n",
    "    with tf.name_scope(name):\n",
    "        n_inputs = int(X.get_shape()[1])\n",
    "        stddev = 2 / np.sqrt(n_inputs)\n",
    "        init = tf.truncated_normal((n_inputs, n_neurons), stddev=stddev)\n",
    "        W = tf.Variable(init, name=\"kernel\")\n",
    "        b = tf.Variable(tf.zeros([n_neurons]), name=\"bias\")\n",
    "        Z = tf.matmul(X, W) + b\n",
    "        if activation is not None:\n",
    "            return activation(Z)\n",
    "        else:\n",
    "            return Z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 먼저 레이어의 이름을 사용하여 name scope를 만든다.이 뉴런 레이어에는 필요한 모든 계산 노드가 포함된다. 선택 사항이지만 노드가 잘 구성되어 있으면 그래프가 TensorBoard에서 훨씬 더 멋지게 보입니다. \n",
    "2. 다음으로, 입력 수를 얻는데, 입력 행렬의 모양을 찾아 두 번째 차원의 크기를 가져 온다.  (첫 번째 차원은 인스턴스 용입니다). \n",
    "3. 다음 세 줄은 가중치 행렬을 보유 할 W 변수를 작성한다. 각 입력과 각 뉴런 사이의 모든 연결 가중치를 포함하는 2D 텐서가 된다. 따라서 그 모양은 (n_inputs, n_neurons)가 된다. 표준 편차가 2 / ninputs 인 truncated normal (Gaussian) 분포를 사용하여 무작위로 초기화됩니다. 이 특정 표준 편차를 사용하면 알고리즘이 훨씬 빨리 수렴하는 데 도움이 된다 (이는 신경 네트워크의 효율성에 엄청난 영향을 끼친 작은 조정 중 하나). 경사 하강 알고리즘이 대칭문제를 피하기 위해 모든 숨겨진 레이어에 대해 연결 가중치를 임의로 초기화하는 것이 중요하다 . \n",
    "4. 다음 줄은 0으로 초기화 되는 bias  b 변수를 만듭니다 (여기서는 대칭 문제 없음) 각 뉴런 당 하나의 바이어스 매개 변수가 있다. \n",
    "5. 그런 다음 z = X · W + b를 계산하기 위한 부분 그래프를 만든다. 이 벡터화 된 구현은 배치의 모든 인스턴스에 대해서 레이어와 뉴런 각각 입력의 가중치 합계와 바이어스 합을 효율적으로 계산한다.\n",
    "6. 마지막으로 활성화 매개 변수가 \"relu\"로 설정된 경우 코드에서 relu (z) (즉, max (0, z))를 반환하거나 아니면 z를 반환한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"dnn\"):  # deep neural network를 만들자\n",
    "    hidden1 = neuron_layer(X, n_hidden1, name=\"hidden1\",  # 첫번째 hidden layer에서는 X를 input으로 받는다.\n",
    "                           activation=tf.nn.relu)\n",
    "    hidden2 = neuron_layer(hidden1, n_hidden2, name=\"hidden2\",  # hidden1을 ouput을 input으로 받는다.\n",
    "                           activation=tf.nn.relu)\n",
    "    logits = neuron_layer(hidden2, n_outputs, name=\"outputs\") # ouput layer에서는 hidden2을 ouput을 input으로 받는다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 명확하게 보기 위해 name scope를 사용했다. \n",
    "- logits는 softmax 활성화 함수를 거치기 전에 신경망의 출력입니다. 최적화를 위해 나중에 softmax 계산은 나중에 처리 할 것입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y,\n",
    "                                                              logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 이제 우리는 신경망 모델을 사용할 준비가되었습니다. 우리는 이를 훈련시키는 데 사용할 Cost Function을 정의해야 한다.\n",
    "- 교차 엔트로피는 목표 클래스에 대해 낮은 확률로 추정하는 모델에 불이익을 준다. TensorFlow는 교차 엔트로피를 계산하는 몇 가지 기능을 제공하는데, 우리는 `sparse_soft max_cross_entropy_with_logits ()`를 사용한다.\n",
    "- `sparse_soft max_cross_entropy_with_logits ()`: \"logits\"(즉, softmax 활성화 함수를 거치기 전에 네트워크의 출력)를 기반으로 크로스 엔트로피를 계산합니다. 0에서부터 클래스 수 - 1 (우리의 경우 0에서 9)개의 라벨링을 한다. 이것은 각 인스턴스 마다 교차 엔트로피를 포함하는 1D 텐서를 출력한다. 이때 TensorFlow의 `reduce_mean ()` 함수를 사용하여 모든 인스턴스에 대한 평균 교차 엔트로피를 계산할 수 있다.\n",
    "> `sparse_softmax_cross_entropy_with_logits ()` 함수는 softmax 활성화 함수를 적용한 다음 크로스 엔트로피를 계산하는 것과 동일하지만보다 효율적이며 logits = 0 과 같은 사례를 적절하게 처리합니다. 따라서 softmax 활성화를 적용하지 않았다.\n",
    "`softmax_cross_entropy_with_logits ()`라는 또 다른 함수는 이 함수는 one-hot 벡터의 형태로 레이블을 취합니다 (int에서 0에서 클래스 수 빼기 대신)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "신경망모델, 비용함수를 정의했으니, 비용 함수를 최소화하기 위해 모델 매개 변수를 조정할 `GradientDescentOptimizer`을 정의하자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 구축 단계의 마지막 중요한 단계는 모델 평가 방법을 지정하는 것이다. 여기서는 단순히 성능 척도로서 정확도를 사용한다.\n",
    "- 첫째, 각 인스턴스에 대해 가장 높은 logit 값이 target 클래스에 해당하는지 여부를 확인하여 신경망의 예측이 올바른지 확인한다. \n",
    "- 이를 위해 in_top_k () 함수를 사용하자. 부울 값으로 가득 찬 1D 텐서를 반환하므로써 부울을 부동 소수점 형으로 캐스팅 한 다음 평균을 계산해야한다. 이렇게하면 네트워크의 전체적인 정확성을 얻을 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    accuracy_summary = tf.summary.scalar('accuracy', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "평소처럼 모든 변수를 초기화하는 노드를 만들어야하며, 훈련 된 모델 매개 변수를 디스크에 저장하는 `Saver`도 만들자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "입력 및 대상에 대한 place holder을 만들었고, 신경 층을 만드는 함수를 만들고 DNN을 만드는 데 사용했으며, 비용 함수를 정의했다. 최적화 도구를 만들고 마지막으로 성능 측정 값을 정의했습니다. 이제 실행 단계로 넘어가자"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execution Phase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- MNIST를 로드를(ScikitLearn 대신) TensorFlow로 데이터를 가져 와서 크기를 조정하고 (0에서 1 사이), 셔플 링하고. 한 번에 하나의 미니 배치를 로드하는 간단한 함수를 제공하니 이용해보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 실행하려는 에포크의 수와 미니 배치의 크기를 정의하자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_epochs = 40\n",
    "batch_size = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 학습을 시키자"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 이 코드는 TensorFlow 세션을 열고 모든 변수를 초기화하는 init 노드를 실행한다. 그런 다음 기본 학습 루프를 실행한다. \n",
    "- 각 에포크마다 코드는 학습 데이터 세트 크기에 해당하는 미니 배치를 반복 수행 한다. \n",
    "- 각 미니 배치는 next_batch () 메소드를 통해 가져오고 코드는 학습 작업을 실행하면서, 현재 미니 배치 입력 데이터와 target을 제공한다.\n",
    "- 다음으로, 마지막 에포크에서, 코드는 마지막 미니 배치와 전체 학습 세트에서 모델을 평가하고 결과를 출력한다.\n",
    "- 마지막으로 모델 매개 변수가 디스크에 저장된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_train = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_test = accuracy.eval(feed_dict={X: mnist.test.images,\n",
    "                                            y: mnist.test.labels})\n",
    "        print(epoch, \"Train accuracy:\", acc_train, \"Test accuracy:\", acc_test)\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the Neural Network\n",
    "- nn을 학습시켰으니, 이제 예측하는데 써먹어보자\n",
    "- 동일한 구성 단계를 재사용 할 수 있지만, 다음과 같이 실행 단계 아래와 같이 변경시켜보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"./my_model_final.ckpt\") # or better, use save_path\n",
    "    X_new_scaled = mnist.test.images[:20]\n",
    "    Z = logits.eval(feed_dict={X: X_new_scaled})\n",
    "    y_pred = np.argmax(Z, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 먼저 디스크에서 모델 매개 변수를 로드하자.\n",
    "- 그런 다음 분류 할 새 이미지를로드합니다. \n",
    "- 교육 데이터의 경우와 feature scaling 을 적용해야 한다. (이 경우 크기를 0에서 1로 조정). 그런 다음 코드는 logits 노드를 평가합니다. \n",
    "- 모든 예상 클래스 확률을 알고 싶다면 softmax () 함수를 logits에 적용해야하지만,\n",
    " - argmax () 함수를 수행함으로써 클래스를 예측하려는 경우 가장 높은 logit 값을 가진 클래스를 선택할 수 있다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Predicted classes:\", y_pred)\n",
    "print(\"Actual classes:   \", mnist.test.labels[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output, Image, display, HTML\n",
    "\n",
    "def strip_consts(graph_def, max_const_size=32):\n",
    "    \"\"\"Strip large constant values from graph_def.\"\"\"\n",
    "    strip_def = tf.GraphDef()\n",
    "    for n0 in graph_def.node:\n",
    "        n = strip_def.node.add() \n",
    "        n.MergeFrom(n0)\n",
    "        if n.op == 'Const':\n",
    "            tensor = n.attr['value'].tensor\n",
    "            size = len(tensor.tensor_content)\n",
    "            if size > max_const_size:\n",
    "                tensor.tensor_content = b\"<stripped %d bytes>\"%size\n",
    "    return strip_def\n",
    "\n",
    "def show_graph(graph_def, max_const_size=32):\n",
    "    \"\"\"Visualize TensorFlow graph.\"\"\"\n",
    "    if hasattr(graph_def, 'as_graph_def'):\n",
    "        graph_def = graph_def.as_graph_def()\n",
    "    strip_def = strip_consts(graph_def, max_const_size=max_const_size)\n",
    "    code = \"\"\"\n",
    "        <script>\n",
    "          function load() {{\n",
    "            document.getElementById(\"{id}\").pbtxt = {data};\n",
    "          }}\n",
    "        </script>\n",
    "        <link rel=\"import\" href=\"https://tensorboard.appspot.com/tf-graph-basic.build.html\" onload=load()>\n",
    "        <div style=\"height:600px\">\n",
    "          <tf-graph-basic id=\"{id}\"></tf-graph-basic>\n",
    "        </div>\n",
    "    \"\"\".format(data=repr(str(strip_def)), id='graph'+str(np.random.rand()))\n",
    "\n",
    "    iframe = \"\"\"\n",
    "        <iframe seamless style=\"width:1200px;height:620px;border:0\" srcdoc=\"{}\"></iframe>\n",
    "    \"\"\".format(code.replace('\"', '&quot;'))\n",
    "    display(HTML(iframe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "show_graph(tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Fine-Tuning Neural Network Hyperparameters\n",
    "- randomized search\n",
    "- Oscar "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Number of Hidden Layers\n",
    "- 깊은 네트워크는 얕은 네트워크보다 파라미터 효율성이 훨씬 높다. 얕은 네트워크보다 지수 함수 적으로 더 적은 뉴런을 사용하여 복잡한 함수를 모델링 할 수 있으므로 학습이 훨씬 빨라졌습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of Neurons per Hidden Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Activation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using dense() instead of neuron_layer()\n",
    "Note: the book uses tensorflow.contrib.layers.fully_connected() rather than tf.layers.dense() (which did not exist when this chapter was written). It is now preferable to use tf.layers.dense(), because anything in the contrib module may change or be deleted without notice. The dense() function is almost identical to the fully_connected() function, except for a few minor differences:\n",
    "several parameters are renamed: scope becomes name, activation_fn becomes activation (and similarly the _fn suffix is removed from other parameters such as normalizer_fn), weights_initializer becomes kernel_initializer, etc.\n",
    "the default activation is now None rather than tf.nn.relu.\n",
    "a few more differences are presented in chapter 11."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_inputs = 28*28  # MNIST\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_outputs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, name=\"hidden1\",\n",
    "                              activation=tf.nn.relu)\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, name=\"hidden2\",\n",
    "                              activation=tf.nn.relu)\n",
    "    logits = tf.layers.dense(hidden2, n_outputs, name=\"outputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_epochs = 20\n",
    "n_batches = 50\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_train = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_test = accuracy.eval(feed_dict={X: mnist.test.images, y: mnist.test.labels})\n",
    "        print(epoch, \"Train accuracy:\", acc_train, \"Test accuracy:\", acc_test)\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "show_graph(tf.get_default_graph())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
